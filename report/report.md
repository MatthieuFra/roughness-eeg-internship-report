[[Projets]], [[Research]], [[Cognitive science]], [[Neurosciences]]

#cognitiveScience #homework #science #writing

---

# Assessing the link between neurophysiological and psycho-affective responses to acoustic roughness
Author: Matthieu Fraticelli ![[Pasted image 20230531173232.png|20]]; [ORCID-ID](https://orcid.org/0000-0002-5025-9913).
*Ecole Normale Supérieure, Department of Cognitive Studies / Hearing institute, Auditory cognition and communication team.*

![[Capture d’écran 2023-03-21 à 12.45.19.png|600]]

---

**Preamble note :**
All this report is available online, at this link, using a web display of markdown file. Some links are used in effort to provide more interactivity throughout the document and with external ressources. If you read this report on PDF, the links will be active. If you read it on paper, you can report to the web section to get all the corresponding links. 

**GitHub repository :** https://github.com/MatthieuFra/roughness-eeg-internship-report

**OSF Project :** https://osf.io/5tdes/


---
# Acknowledgements

I want to thanks all the people that made possible this experiment and this internship.

Emma Ducos, for all her advices and all the good jokes and happyness.

Luc Arnal, for his advices, tutoring on research in general and for having me in his team.

Tanguy Delmas, Keith Dolling, Nadège Marin and Mathieu Pham VanCang for their welcome, as well as all the stimulating discussions, scientific or not.

And all the others persons from the hearing institute that helped me during my stay.

For the Ecole Normale Superieure, I want to thanks Daniel Pressnitzer for his advices and tutoring, Yves Boubenec and Camille Lemarchand for the organization part.

And also, my family for their advices and patience, and all the other Cogmaster students for the discussions that helped the project.


# <u>Abstract</u> 


Our research aimed to determine if autistic traits in neurotypical population, is linked with typical responses to roughness.

We performed an experiment (n=26) measuring both behavioural and neurophysiological responses, using two EEG headsets, while listening to clicktrains of different frequencies. The participant had to rate his aversion towards each sounds using a slider to respond. They completed an autism questionnaire and anxiety questionnaires before and after the procedure.

However, our results are insufficient to clearly establish that autistic trait is correlated with stereotyped responses to roughness, both behaviourally and with EEG. These difficulties are likely due to an inappropriate frequency range for the stimuli to elicit distinct responses. 
Our comparison between the two devices let unclear the usability of wearable headsets for such recordings. 

Further experiments are needed in order to disentangle this mixed up results, and determine whether autism isn't leading to a differenciation of response to acoustic roughness, or if 



# <u>Introduction</u>

Communication is a fundamental aspect of human interaction, enabling the exchange of information, ideas, and emotions between individuals. Audition, which refers to the sense of hearing, plays a crucial role in our ability to communicate effectively. It involves the perception and interpretation of auditory stimuli, such as speech, music, and environmental sounds.

But, it seems that some sounds makes us react differently than others. Hearing babies crying, screams, or alarms, and other particular sounds have the capacity of making us react strongly with aversion. Theses reactions are an interesting way to probe our brains, and get a better understanding of our auditory perception, and more generally our cognition. 

Theses sounds share a common ground between them : roughness. Roughness refers to the perceived quality of a sound that is characterized by a sensation of rapid fluctuations in sound intensity or frequency. It can be described as a sensation of auditory "aggresivity" or "irregularity"
Acoustically, roughness is defined by rapid temporal fluctuations, and is characterized by frequencies around the boundary between discrete and continuous sounds, falling between the perception of rhythms, sound events, and the perception of pitch or other continous vocalizations.

![[Pasted image 20230608094055.png|250]]
**Fig:** Modulation power spectrum (MPS) of a sentence represented in black. Roughness is contained in the orange zone (From Arnal, XXXX)


Studies have put into light that our perception to roughness is a very interesting phenomenon. We react quicker than for other sound perception, and by recruiting wider neural circuits across the brain (solliciting the amygdala for instance), in a much more general way than only classical auditory related areas. This stereotyped responses makes us more performant to detect these particular sounds. It makes sens from a evolutive point of view that this feature was selected through natural selection, since being better at detecting rough sounds, can help to escape from a predator, or to help relatives. 

Since roughness perception recruit wider neural circuits, it has been postulated that using electroencephalography (EEG) to study brain responses to roughness, can be a good proxy of the integrity of theses large networks. Using EEG measures of the perceiving brain to roughness, we can assess dynamical properties of the underlying networks. 

Neurodegenerative diseases are known to alter neuronal circuits and therefore, brains responses. With event related potentials, we can observe an alteration in evoked components, with a reduced response for people affected with such diseases. Alzheimer disease for example, has been linked with a different brain response regarding roughness perception (XXX). Knowing that these diseases are linked with such responses, allow this technique to be used as a future diagnosis tool, able to probe early the integrity of large neuronal pathways, affected by neurodegenerative process. 

Autism spectrum disorder (ASD) is known to be neurodevelopmental disease showing an altered neuronal activity, with a enhanced local connectivity in small areas, but with reduced connectivity between wider brain areas. 
The relation between ASD and auditory perception has already been explored, and we know that ... 
However, research investigating ASD's responses to roughness are lacking. Does autism, as other disease involving modifications or alteration of neural pathways, shows a different pattern of response to roughness ? 
This question is our main interrogation, and what we'll be trying to explore in this internship. 

In addition, the patients with such diseases are often facing constraints to go into hospital, and performing studies or diagnosis can quickly become challenging. To avoid such difficulties, it's tempting to use recents technical advances, as wearable EEG, in order to perform such recordings at home in a more confortable way for patients. 

But theses new devices are still under development, and their ability to provide clinical-grade recordings are yet subject to debate. In order to ... we decided to conduct a benchmark within our experiment, to compare a classic EEG headset, with a wearable one, and assess its capacity to perform such recordings. A previous internship in the team used the same wearable headset, but results were mixed. 

So we decided to conduct another experiment, assessing the link between autsim towards neurophysiological and psycho-affective responses to acoustic roughness. This time with more participants to get a better statistical power, and also to perform the recording conjointly with a classic clinical grade EEG in order to compare more precisely the two. 
This experiment being a replication of related research in the team, it will be necessary to confirm that we obtain the same pattern of response to roughness, as previously found. 






# <u>Material and method</u>


## Participants

We have conducted this experiment with a group of participants (n = 26) recruited using the Expescience mailing list of the RISC. The group of participant was composed of X females and X males, with an age range from 20 to 66 (SD =13.0). All the participants were normo-hearing and neurotypical. 
But due to time constraints it wasn't possible to perform a tonal audiogram before each session, so we aren't completely certain of the audiogram of each participant. Also, the neurotypicality wasn't assessed neither, but an autistic trait questionnaire was passed. 

This experiment benefited of the ethics approval (CPP Approval 28/06/2022, [AUD_SEQ_4F](https://matthieufra.github.io/roughness-eeg-internship-report/online_interactive_content/pdfs/1.pdf)), and the participants were paid 20 euros per hour for their participation. The experiment lasted one hour. 


![[Pasted image 20230608085936.png|200]]
**Fig:** Descriptive statistics of participants age


![[Pasted image 20230608090011.png|300]] 
**Fig:** Descriptive statistics of gender

![[Pasted image 20230531172600.png|300]]

**Fig:** Box plot and violin of age distribution 


![[Pasted image 20230531202138.png|380]]
**Fig:** Bar plot of age by gender repartition


![[Pasted image 20230608090101.png|350]]
**Fig:** Descriptive statistics of handedness. 



## Stimuli 

The audio stimuli presented were clicktrains of different frequencies. Two clicks trains were presented one after the other. 

![[Pasted image 20230607151736.png|700]]
**Fig:** Schematic of two click trains of frequencies *f* and *f'* , a single click is represented on the right. 


A click is an audio impulse, with a vertical front, and a vertical decrease that produce a click, as illustrated in the figure above. The click trains consist in several clicks, presented periodically at a given frequency. It's to that frequency that we refer when we talk of the click trains frequencies. The duration of each click trains is one second, and the inter stimuli interval (ISI) remains constant between the different trains (One second also). 

For this experiment, we used click trains of 10Hz, 20Hz, 30Hz, 40Hz, 60Hz, 80Hz and 90Hz. We didn't go higher due to our bandwith limitation for the EEG recordings of the Q+, indeed, its high-end internal bandwith is at 100Hz. The different sounds are available in the supplementary material, and can be listened to by clicking [**here**](https://matthieufra.github.io/roughness-eeg-internship-report/online_interactive_content/online_interactive_content.html). All the stimuli were displayed using a PsychoPy script. 

The loudness was controlled and the sound chain was calibrated using an artificial hear. Each participant could set the volume at the start of the experiment, but could only go higher. The instructions were "*Set the volume to a supportable level*". Then we could get an estimation of the loudness of the sounds for each participants, knowing the calibrated point and the precise increase in PsychoPy. 

To generate the different click trains, we used a MatLab code available in the supplementary material. 


## EEG Headsets

### *Brain product R-Net*

The R-net from BrainProduct is a classical clinical grade EEG with 32 wet passive electrodes using a solution of water and potassium chloride (KCl). The choosed sampling rate of the R-Net was 1000Hz. The reference was a common averaged reference across all electrodes.
The main advantage of using such an headset is the facility to install it, without using gel for each electrodes. It's quicker and more convenient for participant to use. However, since it's working using water, the total duration of the experiment is a limiting factor. This wasn't an issue for us since the experimental procedure was 30min. 

![[Pasted image 20230601105325.png|400]]

**Fig:** R-Net on a participant's head.


![[Pasted image 20230531201920.png|300]]

**Fig:** Placement of the 32 electrodes (gray dots) according to the extended 10-20 system. 

### *MBT Q+*

The Q+ from the company MyBrainTechnology is a wearable EEG headset, mounted on headphones, using 6 dry actives electrodes. The sampling rate of the Q+ is 250Hz.

Four electrodes are disposed on the head, respectively on *P3*, *P4*, *AF3* and *AF4* location of the 10-20 location system. Two reference electrodes (*A1*, *A2*) are localized on the side of the headphone, behind the hear, on the mastoïd bone. A1 was used as the ground electrode, and A2 as the reference.


![[Pasted image 20230531195309.png|600]]
**Fig:** A: Schematic of electrode placement according to the international 10-20 system, adapted from Luck, 2014. The black circle represents a skull seen from above, with the front and back as indicated. The electrodes circled in red are those present on the Q+ helmet. B: Q+ helmet and name of the electrodes in white (top), start-up logo (bottom). (From Jeanne's previous internship)


## Experimental design 

Different sensors are placed on the participant. First, we install the facial electromyography (EMG), in order to record the activity of the orbicularis oculi (Boxtel, 2010) (Dimberg, 2002). Two electrodes were posed below the left eye of the participant, and the reference was placed on the right cheek, to avoid muscular activity.

![[Pasted image 20230608144932.png|300]]
**Fig:** Facial reactions measured with facial EMG (adapted from Boxtel 2010)


The electrocardiogram (ECG) consisted in three electrodes, placed on the sites RL, LA and LL (respectively right leg, left arm, and left leg) as shown in the figure below.

![[Pasted image 20230608150203.png|170]]

**Fig:** Placement of standard ECG using 3 electrodes (adapted from Hakimi, 2018)

After the installation of these electrophysiological sensors, we placed the R-Net on the participant, using the correct size cap, and adjusting the electrodes placement in order to get Cz at the center of the Nasion-Inion distance, and the headset to be symmetrical on both sides. 

On top of that, we could finally put the Q+, adjusting A1 and A2 electrodes to be in contact with the back of the hear, and the others electrodes to not touch any of the R-Net electrodes. 


![[Pasted image 20230601155942.png|400]]

**Fig:** Adjustment of impedance  


![[Pasted image 20230531220604.png|600]]
**Fig:** Diagram of the experimental design. 1: Q+ Headset, 2: R-Net Headset, 3: EMG Electrodes



## Experimental procedure

The experiment consisted in an oddball paradigm, consistent with previous experiments done in the team (Jeanne, XXX). The instructions were the following : 

> *Vous allez entendre des sons.
> Lorsqu'une échelle apparait, veuillez noter les sons que vous avez entendus parmi :
> "neutre", "acceptable", "gênant", "insupportable", "horrible"
> Lorsque vous êtes prêts, faites un clic droit avec la souris.*

We remind the participants to avoid as possible to blink during the acquisition (in particular during the sound presentation) and to clench the jaw to avoid muscular artifacts. 

The participant was told to sit in an anechoïc chamber, to ensure sound isolation from the outside. They complete a questionnaire investigatin autistic trait, the QA questionnaire; and two scales for anxiety: the STAI-T (for the anxiety trait) and the STAI-S (for the present state anxiety) for the *before* condition.

First, they have to close their eyes for about two minutes, in order to perform a closed-eyes resting state. After this brief phase, the trials begin. We present to them two clicktrains, one after the other. The first clicktrain presented is randomly choose from the 7 possible frequencies. Then, after the ISI of 1s, the second clicktrain is presented. In 80% of the times, the second clicktrain is the same frequency of the first one. In 20% the times, it's another frequency, chose between the left possibilities. 150 pairs of clicktrains are presented, for a total of 300 clicktrains. After the presentation of the second stimuli, participant have to write their aversion using a continuous scale, with a slider. The participant with the mouse can choose between 5 levels, from "Neutre" to "Horrible". 

After that, they complete a second time the STAI-S, for the *after* condition. 

The more detailed operational version we used is here : [[IDA Experiment Protocol]].


![[Pasted image 20230531202803.png|400]]

**Fig:** Diagram of the experiment


The complete experiment procedure last 30 minutes, with an additional 30 minutes of questionnaires, and installation of the EEGs, ECG and EMG devices. 

![[Pasted image 20230601160121.png|400]]

**Fig:** Participant doing the experiment


## Data acquisition 

The data acquisition was performed using two computers, one tablet for the Q+, and the ActiChamp for the R-Net. The stimulation computer was the host of the PsychoPy experiment, to display the stimuli but also to record the behavioral data (slider responses) . The recording computer was in charge of the acquisition of the EEG data for the two headsets. 



![[Pasted image 20230531202521.png|700]]

**Fig:** Diagram of the experimental design. 


In order to carefully control the synchronization between all the devices, we used LabStreamingLayer (LSL). LSL is a software used to stream, and synchronize datas stream from diverse sensor hardware. It allows to unify the acquisition of different streams (as the Q+ and the R-Net) and more importantly to ensure through the network a time-synchronization between the streams.

The recording computer was designed to be the host for the LSL network. The ActiChamp was connected to it using the BrainProduct LSL software connector, specially designed for that, and the Tablet used for the Q+ was using a dedicated app to create a LSL stream, designed by MyBrainTechnology for our experiment. 
Then, using the LabRecorder of LSL, we performed the recording on the dedicated computer. In order to gather triggers to synchronize the different streams, the audio stimuli went in an auxilliary channel of the ActiChamp amplifier, passing through a StimTracker, that sent triggers on the onset of each click presented. 

Data were aquired in BIDS format, with a sampling rate of 1000Hz for the R-Net and 250Hz for the Q+, in the .xdf format provided by LSL. 

## Behavioural analysis 

The behavioural analysis concerns both the autistic trait and anxiety questionnaires, in addition to the aversion response of the slider. In addition to simple descriptive statistics of each scores for each questionnaires, we decided to perform a regression analysis between the aversion and the autistic score obtained. Also, in a more exploratory way, we investigated the possible correlation between QA and STAI-T scores. Since these are published scales, we just had to use the scoring tools provided. 
For PsychoPy, we gathered the slider responses for each participants, knowing to which stimuli presented it corresponds. 

## EEG analysis

Data analysis was performed on the data after its conversion to FIFF format. 

We used MNE-Python for the analysis of EEG data, with Jupyter NoteBooks for a better usability (But if we had to perform with more participants we would had create a dedicated pipeline in another Python editor, to automatize more easily the processing, but for this report and to share the code, it was graphically more interesting to use Jupyter NoteBooks). 

The analysis of the EEG data consisted in different operations, including some pre-processing and the main analysis. The preprocessing consisted in the following operations : 

1. Filtering (Bandwith and notch filter of the line noise), 
2. Independant component analysis (ICA),
3. Epoching the signal.

### Filtering 

Filtering consist in removing the unwanted frequencies, in order to remove noise. We filtered the bandwith, between 0.1Hz to 500Hz for the R-Net, and between 0.1Hz to 100Hz for the Q+. Then, a notch filter at 50Hz (including filtering for each harmonics of AC power, going to the limit of the bandwith) is applied, to remove the power line noise.

**A:** 
![[1.png|350]]

**B:** 
![[2.png|350]]

**Fig:** Raw plot of EEG signal of the R-Net, **A:** without any preprocessing, **B:** with notch filter of the line noise and band-with limitation. 


### Independant component analysis 

The independant component analysis (ICA) of the signal, is a signal processing method to perform a linear decomposition method of the signal, in order to separate independant sources linearly mixed in several sensors. In our case, we used it to remove blinks artifacts and others eye movements. We plotted the ICA components resulting of the analysis, seeking for the ones containing the blinks and eye artifacts. After that, we checked with a topographical plot of the components, if they were located close to the eyes. 


![[3.png|270]]

**Fig:** Example of an eye-blink artefact, at 155s, on almost all channels. 


![[Pasted image 20230608155240.png|300]]
**Fig:** Example of ICA components: ICA0001 contains eye information

We used the electrodes Fp1 and Fp2 to fit the ICA model on the raw data. The model was fitted on the raw data with a high pass at 1Hz, because ICA react badly to low frequencies (Luck, XXXX). We first wanted to use the facial EMG to remove eye movements, but due to time constraints it wasn't possible to perform testing beforehand. 


This preprocessing step was only performed on the R-Net, indeed, with only 4 channels of interest, the Q+ lacks sources to perform a meaningful ICA. 

![[ica2.png|300]]
**Fig:** ICA components, showing the third component containing the blinks. 


### Epochs 

Epoching the data, is the fact of going from a continuous signal to a event segmented one. Indeed, we performed in the experiment many trials, that need to be grouped together. In order to segment correctly the signal to correspond to the trials, we use the triggers that we used in the experiment. 
Since we didn't manage to make PsychoPy send triggers directly to LSL on the recording computer, we had to use as described ealier, a StimTracker in order to create onset events for each clicks. From this STIM channel, we've got a lot of events, corresponding to each click. Since we're only interested in the onset of each clicktrains, we used a code to only take those events into account. 


![[MicrosoftTeams-image (7).png|300]]
**Fig:** In blue, the actual events recorded for each clicks, in black, the result of parsing onsets with our code. 

![[MicrosoftTeams-image (6).png|100]]
**Fig:** Verification of the correspondance between the obtained onset event of interest, and the actual event.

After the extraction the the events, we epoched the signal using an interval of -0.5ms to 1.5ms around the event, and applying a baseline from -0.3 to 0.1 around the event.

Once the signal is segmented, we check the epochs, both visually and automatically, to remove the bad ones from the analysis. A bad epoch is an epoch containing a residual blink or other kind of artifacts. We used an MNE function to automatically drop bad epochs, resulting in approximately 5% loss for the R-Net. A visual control is then performed, to remove epochs that could pass the filter, but still remains unusable. 

Concerning the Q+, due to an extremely bad signal, we lost more than 10% of the epochs, with a lighter threshold, to avoid loosing to much statistical power. 

### Main analysis 

The main analysis consisted in getting the evoked responses of the two headsets. In order to do that, we have to combine all the epochs, and average over them to get an ERPs. 
We wanted to do both grand-average ERPs for both headsets, and then go into more precise evoked potentials, segregating regarding the order of presentation and by frequencies. 
Also, in order to investigate in more depth the links with autism, we created ERPs by grouping the participants in a low-autism group and in a high-autism group. 

For the electrophysiological data, we originally wanted to use the Python toolbox for neurophysiological signal processing NeuroKit2. To perform analysis on ECG data and EMG data. But due to time constraints, we couldn't use the electrophysiological data. This dataset will be used later for another experiment in the team. 


# <u>Results</u>

## Questionnaires responses are standard and consistent with a neurotypical population  

The overall data from the questionnaires are the following : 

### *Autistic trait Questionnaire (QA)*

For the Autistic trait questionnaire (QA), we found that our group have a mean score of 14.0 (SD = 5.56). But still, the distribution seems to be compatible with a bi-modal one, allowing to split the group between high-autism and low-autism participants, in order to further evaluate the responses to roughness. The normative data for the questionnaire indicate a significative deviation from the norm when above 35. No participants felt in that category. 

![[Pasted image 20230608090213.png|150]]
**Fig:** Descriptive statistics of autism questionnaire

![[Pasted image 20230601121749.png|300]]
**Fig:** Density plot of QA scores


### *STAI-T*

The anxiety scale for the general condition (the participant had to respond about their state in general) performed on the overall group of participants have a mean score of 40.5 (SD = 9.25).


![[Pasted image 20230601122002.png|300]]
**Fig:** Density plot of STAI-T scores


The density of the STAI-T scores compared to QA is not bimodal and doesn't allow to differentiate in two subsets.

### *STAI-S Before experiment*

The anxiety scale for the state condition (the participant had to respond about their state in the present moment) performed on the overall group of participants have a mean score of 29.7 (SD = 7.89).

### *STAI-S After experiment*

The anxiety scale for the state condition (the participant had to respond about their state in the present moment) performed on the overall group of participants have a mean score of 32.1 (SD = 9.81).


![[Pasted image 20230608090237.png|300]]

**Fig:** Descriptive statistics of the Anxiety scale


![[Pasted image 20230601143144.png|450]]

**Fig:** Normative data for STAI-S and STAI-T (adapted from Spielberger, 1983)

The standard mean for normative use of the STAI-S is approximately 35.22 (SD = 10.30), and for the STAI-T is approximately 34.57 (SD = 9.02 ), averaged over all ages and both gender. 
Our group doesn't statistically differ from this value, neither for STAI-S or STAI-T. 

A t-test between the condition before and after show that there's no significant difference between the two (s = -1.09 p < 0.142). 


![[Pasted image 20230608090259.png|400]]

**Fig:** Plot of t-test of difference between *before* and *after*


![[Pasted image 20230608090335.png|400]]

**Fig:** Plot of difference between *before* and *after* condition


Also, we was interested to see if there was any correlation between QA and STAI-T scores, since it's pointed by studies that although anxiety is not considered a core feature of ASD, 40% of young people with ASD have clinically elevated levels of anxiety or at least one anxiety disorder, including obsessive compulsive disorder.

Unfortunately, our results doesn't show this kind of relation overall (r = 0.313, p = 0.119). 

![[Pasted image 20230608090354.png|250]]
**Fig:** Correlation matrix of STAI-T and QA 


![[Pasted image 20230601130706.png|300]]

**Fig:** Regression plot between STAI-T (x-axis) and QA (y-axis)



![[Pasted image 20230608104019.png|450]]
**Fig:** Regression plot between STAI-T (x-axis) and QA (y-axis) with gender distinction. 


Distinguishing between gender doesn't provide more insights, except a trend for female having an higher anxiety score than male.


## Behavioural data doesn't correspond to the expected pattern regarding autistic trait

Since the participants had to evaluate their subjective aversion with a slider, we could use the PsychoPy output. Unfortunately, a software issue prevented us to collect all the CSV files from PsychoPy, and we had to write scrapping code to extract informations from log files and PSYDAT files. 

Doing so, we created one file per participant, displaying the sound played, and the corresponding slider response of the participant. 

It allowed us to plot the evolution of aversion, regarding frequencies of stimulation as shown in the figure below.


![[Aversion_freq_nobaseline.png|400]]
**Fig:** Relation between aversion (y-axis) and frequencies (x-axis). Each red curves is a participant, the thick black curve is the general mean of each participants.



![[Aversion_freq_baseline.png|400]]
**Fig:** Relation between aversion (y-axis) and frequencies (x-axis). Each red curves is a participant, the thick black curve is the general mean of each participants. Each curve is normalized. 



![[Aversion_freq_QA_nobaseline.png|400]]
**Fig:** Relation between aversion (y-axis) and frequencies (x-axis). Each red curves is a participant in the high QA group, the thick red curve is the general mean of each participants in this group. Each blue curves is a participant in the low QA group, the thick blue curve is the general mean of each participants in this group.



![[Aversion_freq_QA_baseline.png|400]]
**Fig:** Relation between aversion (y-axis) and frequencies (x-axis). Each red curves is a participant in the high QA group, the thick red curve is the general mean of each participants in this group. Each blue curves is a participant in the low QA group, the thick blue curve is the general mean of each participants in this group. All curves have been normalized. 


It appears that this distinction doesn't seems to raise any noticeable differences. 

It may be caused by the artificial grouping we performed, based on a split at the median. In order to further investigate this, we decided to see if there's a correlation between QA scores and aversion, for each frequency, as shown in the figure below. 

We ended up with 14 subplots, showing Pearsons's *r* regression coefficient and a regression line, for each frequency, with and without normalization. The figures are displayed in the next pages. And to get a complete look on these relations, we decide to do the same for the STAI-T results. 

![[regression_freq_QA_10.png|400]]
**Fig:** Regression plot between STAI-T (x-axe) and QA (y-axe)



![[regression_freq_QA_multi_nobaseline.png]]
**Fig:** Regression plot between QA (x-axe) and aversion (y-axe) for each stimuli frequencies.




![[regression_freq_QA_multi_baseline.png]]
**Fig:** Regression plot between QA (x-axe) and aversion (y-axe) for each stimuli frequencies. With a normalization of each participants.




![[regression_freq_STAIT_multi_nobaseline.png]]
**Fig:** Regression plot between STAI-T (x-axe) and aversion (y-axe) for each stimuli frequencies. 



![[regression_freq_STAIT_multi_baseline.png]]
**Fig:** Regression plot between STAI-T (x-axe) and aversion (y-axe) for each stimuli frequencies. With a normalization of each participants.


We can see that we do not found any interesting correlation coefficient, our maximum being r = *0.13*. With these results, we fail to replicate previous results from XXXX regarding the STAI questionnaire.





## Mixed results for electroencephalography data

Concerning the EEG signal, after conducting all the preprocessing detailed in the Method section, it was possible to average the epochs over all the participants and visualize the events related potentials. 
Due to a difficulty to extract the events ID from the recorded signals, we couldn't group the epochs by frequencies or depending on the order of presentation. We'll go further in more details about that. 

We also chose to restrain our analysis of electrodes of interest : C3, Cz, C4, P3, Pz, P4. We narrowed our analysis to these electrodes due to their ability to probe activity of the brain networks we're interested in.  

### R-Net produces the expected global ERP shape

Despite a noisy signals, resulting in the loss of about 10% of the epochs, we were able to plot the ERP for the R-Net, averaging over all epochs. This means that we combine the epochs of both the first and the second clicktrain presented. 


![[MicrosoftTeams-image (12).png|450]]
**Fig:** ERP of the R-Net Headset for electrodes of interest, from 0.1Hz to 500Hz. 


As we can see, the signal exhibit a lot of noise. But, despite a bad signal-to-noise ratio, we can distinguish classical ERPs components. In order to get a clearer view, we filtered this evoked response with a low-pass at f=30Hz, to smoothe the result. 


![[F1.png|450]]
**Fig:** ERP of the R-Net Headset for electrodes of interest, from 0.1Hz to 30Hz.

We can see more clearly the components : the N200 in particular. The last negative component at approximately 1.20 seconds correspond to the end of the stimuli, leading to the end of corresponding neural activity. 


### Q+ fail to display any classical ERP components 

We applied the same procedure for the wearable headset, the Q+. The corresponding electrodes of the Q+ are P3, P4, AF3 and AF4. 

![[MicrosoftTeams-image (10).png|450]]
**Fig:** ERP of the R-Net Headset for electrodes of interest, from 1Hz to 250Hz. 

As we can see, the result isn't looking like a ERPs. Even a filter at 30Hz as for the R-Net doesn't reveal any clear components. 
When we compare this plot to those obtained in the last internship focused on the Q+, we fail to notice any improvement of it, even with 26 participants. 

![[Pasted image 20230531195536.png|300]]
**Fig:** "ERP" obtained during the last internship using the Q+. (From ,)


### ERPs and autistic trait 

To pursue our exploration of the responses to acoustical roughness in relation to autism, we decided to compare the ERPs of the low autistic trait group, and the high autistic trait group. 

![[ERPScompare.png|450]]
**Fig:** ERPs of the R-Net Headset averaging over electrodes of interest, from 0.1Hz to 500Hz. In blue, the low-QA group and in orange, the high-QA one. 

We can see that again, a lot of noise in high frequencies impairs our visualization. As we did previously, we filter both ERPs with a low-pass of 30Hz. 
![[Figure 12.png|450]]
**Fig:** ERPs of the R-Net Headset averaging over electrodes of interest, from 0.1Hz to 30Hz. In blue, the low-QA group and in orange, the high-QA one. 


It seems that the high-QA group exhibit a lower response than the low-QA one. That could be consistent with the fact that autism is known to be associated with an higher local connectivity, but a weaker neural connectivity across areas. However, the presented ERPs are too noisy to be confident about any results.  


## Electrophysiology data 

The EMG data was meant to perform a principal component analysis for the EEG pre-processing. The EMG could be also used as a proxy of aversion through facial reactions. But we didn't had the time to investigate this part in the present project, and it will be something useful for further experiments, using the dataset. 



# <u>Discussion</u>

### Mixed results 

The presented results are mixed. Indeed, we fail to clearly highlight a noticeable difference of response, both with behaviour responses and EEG responses. 
This can be due to several reasons. 

First, as said before, we chose to present stimuli from 10Hz to 90Hz, for a technical reason : the limitation imposed by the Q+ bandwith, preventing us to go higher than 125Hz approximately (Maximum frequency being the sampling rate divised by two). After 90Hz, the other stimuli was 130Hz, above our limit. So, seeking a compromise between usable data and maintaining the experimental procedure to one hour of duration, we didn't present higher stimuli (130Hzn 180Hz, 240Hz). Also, our original goal was to use auditory steady states responses, that would have been unusable above the bandwith limitation. But due to time constraints and code issues, we had to remove it from the project.

But, it's above 90Hz that we leave the acoustical roughness, to go to a more tonal like perception of the stimuli. And that's with this new domain of perception, that arise different profiles of responses. Limiting the range of stimuli, we have certainly smoothed the results, making them negative or difficult to interpret. Despite the impossibility to use stimuli above 125Hz for the Q+ we should have presented stimuli of higher frequencies, for the R-Net and for the behavioural responses. 

Secondly, our interest was in assessing the relation between response to acoustical roughness and autism. But, our population was neurotypical. The scores from the autistic trait questionnaire aren't outside of the normative data of the tool. We performed a median split following the suggested shape of the score distribution, but it is doubtful that this can be meaningful for the responses. Indeed, the median was m = 15, a pretty medium value without normative signification. 

Thirdly, our incapacity of extracting the events ID from the recordings kept us from distinguish between the different frequencies of the click trains and their presentation, blurs our result. Indeed, we kept the experiment that was used in previous studies to be consistent : an oddball paradigm. But, the instructions weren't clearly stating to which sounds participant had to pay more attention. We think that their response is mainly linked to the second stimuli presented (due to recency effect) and we would compare ERPs of the first and second stimuli. We're lacking this information so far, and so we can't disentangle the mixed results. Also, without the events, we're unable to get the ERPs corresponding to the average response by frequencies. That prevent us to get a more clearer view on the perception of participants. 

Finally, regarding the lack of results for the Q+ ERPs, different reasons can be given. First, it's the second time that an experiment with the Q+ failed to produce ERPs. However, as stated in the pre-print of the construction (Spinelli, XXXX) the headset should supposedly be able of such recordings. Their experimental design is similar to ours, with a simultaneous recording with two EEG headsets. Also, compared to the previous internship, we gather more participants, increasing the statistical power of our results. So why can't we find any ERPs ? Either the headset isn't able to record such signals in lab conditions, or some noise is added on the way. 

One possible explanation of the added noise can be the bluetooth connexion between the headset and the tablet that streams the data. Indeed, bluetooth isn't a reliable synchronised connection. A variability of the data packets sent, would impair massively the ERPs that we're able to get, since we're not able to reduce random noise by summing all the epochs. And since the triggers are received from the STIM channel of the BrainProduct, synchronization accuracy between the Q+ and the tablet is yet unknown. 

Also, MyBrainTechnology recently added the possibility to directly play audio with the headphones. Previously ear plugs were needed. But, by visualizing some epochs for some participants, it seems that some stimuli information *leaked* in the electrodes circuit. Theses part are supposed to be isolated, but we view a strange correspondance between theses artifacts displayed in the figure below, and the stimuli we were using. More analysis have to be conduct to assess the similarity between them in order to conclude, but that may be a major flaw in the device. 


![[MicrosoftTeams-image.png|500]]
**Fig:** Visualization of Q+ epochs, with a stimuli *leak* visible in the AF4 electrode. 


### General difficulties 

Many difficulties raised from this experiment and create opportunities to learn a lot.

First of all, this project is born from the collaboration from a public research institute, the hearing institute from Pasteur, and a private company, MyBrainTechnology. Sure this kind of synergy is an interesting occasion to benefit from each others experience, but it can also be the place for misalignement of interests and temporality. Some problems occured from that, and it was complicated to get support from the company regarding some synchronisation issues with the Q+ but also concerning the accessibility of the data. Indeed, the recordings were originally sent to the private servers of the company, and it was impossible to get access to the raw content. This was fixed with the help of one engineer from the team in the second half of the internship, constraining the time left to perform the experiment. 

Synchronisation was one of the main challenge of the project. Putting together different hardware that doesn't use the same software background was challenging, but we were able to develop an implementation of LabStreamingLayer, that allowed for a precise synchronisation in real time of the different data streams. But, since this implementation was the first time we used that framework, we couldn't benefit from previous knowledge, as much if we could maintain the BrainProducts infrastructure. 


# <u>Conclusion</u>

In conclusion, we can say that despite our hypothesis of a different pattern of responses to acoustical roughness for people with high autistic trait, we weren't able to assert this with certainty, both on a behavioural level with the aversion score, or using the EEG signals. It may be due to the fact that our studied population wasn't displaying enough variability regarding their autistic traits score, which was expected given that they were neurotypical. It can also be due to an inapropriate range of stimuli, lacking to create more contrast between the acoustical roughness domain, and the tonal-like domain. Finally, technical difficulties stopped us to conduct more precise analysis of the EEG datas, by order of presentation and for frequencies in the time allowed. The results may lurk in theses analysis. 

Still, this project was a great occasion to learn about a new subject, new technique and new data to study cognition, and I was very happy to be part of it. 
Thank you for having read this report. 

---

# <u>Bibliography</u>


Arnal, L. H., Flinker, A., Kleinschmidt, A., Giraud, A.-L., & Poeppel, D. (2015). Human Screams Occupy a Privileged Niche in the Communication Soundscape. _Current Biology_, _25_(15), 2051‑2056. [https://doi.org/10.1016/j.cub.2015.06.043](https://doi.org/10.1016/j.cub.2015.06.043)

Arnal, L. H., Kleinschmidt, A., Spinelli, L., Giraud, A.-L., & Mégevand, P. (2019). The rough sound of salience enhances aversion through neural synchronisation. _Nature Communications_, _10_(1), 3671. [https://doi.org/10.1038/s41467-019-11626-7](https://doi.org/10.1038/s41467-019-11626-7)
Iwama, S., Takemi, M., Eguchi, R., Hirose, R., Morishige, M., & Ushiba, J. (2022). _Two common issues in synchronized multimodal recordings with EEG : Jitter and Latency_ (p. 2022.11.30.518625). bioRxiv. [https://doi.org/10.1101/2022.11.30.518625](https://doi.org/10.1101/2022.11.30.518625)

Mackersie, C. L., & Calderon-Moultrie, N. (2016). Autonomic Nervous System Reactivity During Speech Repetition Tasks : Heart Rate Variability and Skin Conductance. _Ear and Hearing_, _37_, 118S. [https://doi.org/10.1097/AUD.0000000000000305](https://doi.org/10.1097/AUD.0000000000000305)

Schneefeld, F., Doelling, K., Marchesotti, S., Schwartz, S., Igloi, K., Giraud, A.-L., & Arnal, L. H. (2022). _Salient 40 Hz sounds probe affective aversion and neural excitability_ (p. 2022.02.26.482077). bioRxiv. [https://doi.org/10.1101/2022.02.26.482077](https://doi.org/10.1101/2022.02.26.482077)

Spinelli, G., Odouard, A., Niérat, M.-C., Campion, S., Bensoussan, M., Grosselin, F., Pandremmenou, K., Breton, A., Raux, M., Attal, Y., Similowski, T., & Navarro-Sune, X. (2020). _Validation of melomind(TM) signal quality : A proof of concept resting-state and ERPs study_. [https://doi.org/10.1101/2020.02.28.969808](https://doi.org/10.1101/2020.02.28.969808)

Tetelepta, S. (2019, janvier 6). Exploring Heart Rate Variability using Python. _Orikami Blog_. [https://medium.com/orikami-blog/exploring-heart-rate-variability-using-python-483a7037c64d](https://medium.com/orikami-blog/exploring-heart-rate-variability-using-python-483a7037c64d)

# <u>Appendix</u> 

## Detailed protocol of the experiment


## HHH

